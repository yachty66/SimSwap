{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yachty66/SimSwap.git\n",
    "!cd SimSwap && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install insightface==0.2.1 onnxruntime moviepy\n",
    "!pip install googledrivedownloader\n",
    "!pip install imageio==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"SimSwap\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader\n",
    "\n",
    "### it seems that google drive link may not be permenant, you can find this ID from our open url.\n",
    "# GoogleDriveDownloader.download_file_from_google_drive(file_id='1TLNdIufzwesDbyr_nVTR7Zrx9oRHLM_N',\n",
    "#                                     dest_path='./arcface_model/arcface_checkpoint.tar')\n",
    "# GoogleDriveDownloader.download_file_from_google_drive(file_id='1PXkRiBUYbu1xWpQyDEJvGKeqqUFthJcI',\n",
    "#                                     dest_path='./checkpoints.zip')\n",
    "\n",
    "!wget -P ./arcface_model https://github.com/neuralchen/SimSwap/releases/download/1.0/arcface_checkpoint.tar\n",
    "!wget https://github.com/neuralchen/SimSwap/releases/download/1.0/checkpoints.zip\n",
    "!unzip ./checkpoints.zip  -d ./checkpoints\n",
    "!wget -P ./parsing_model/checkpoint https://github.com/neuralchen/SimSwap/releases/download/1.0/79999_iter.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \"https://sh23tw.dm.files.1drv.com/y4mmGiIkNVigkSwOKDcV3nwMJulRGhbtHdkheehR5TArc52UjudUYNXAEvKCii2O5LAmzGCGK6IfleocxuDeoKxDZkNzDRSt4ZUlEt8GlSOpCXAFEkBwaZimtWGDRbpIGpb_pz9Nq5jATBQpezBS6G_UtspWTkgrXHHxhviV2nWy8APPx134zOZrUIbkSF6xnsqzs3uZ_SEX_m9Rey0ykpx9w\" -O antelope.zip\n",
    "!unzip ./antelope.zip -d ./insightface_func/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import fractions\n",
    "import numpy as npa\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from models.models import create_model\n",
    "from options.test_options import TestOptions\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from util.videoswap_multispecific import video_swap\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcm(a, b): return abs(a * b) / fractions.gcd(a, b) if a and b else 0\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transformer_Arcface = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!apt install cmake\n",
    "!pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import face_recognition\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop  # Import the face detection class\n",
    "\n",
    "# Initialize the face detection model\n",
    "app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "app.prepare(ctx_id=0, det_thresh=0.6, det_size=(640, 640), mode='None')\n",
    "\n",
    "def is_unique_face(face_encoding, known_faces, threshold=0.6):\n",
    "    \"\"\"Check if the face encoding is unique compared to known faces.\"\"\"\n",
    "    matches = face_recognition.compare_faces(known_faces, face_encoding, tolerance=threshold)\n",
    "    return not any(matches)\n",
    "\n",
    "def process_video(video_path, output_dir, margin):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error: Could not open video.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    known_faces = []\n",
    "    face_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use the face detection model to detect faces\n",
    "        detect_results = app.get(frame, crop_size=640)  # Adjust crop_size as needed\n",
    "        if detect_results is not None:\n",
    "            frame_align_crop_list = detect_results[0]  # Get the aligned crops\n",
    "            for frame_align_crop in frame_align_crop_list:\n",
    "                face_img = frame_align_crop  # This is already the aligned face crop\n",
    "                rgb_face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Get face encodings\n",
    "                face_encodings = face_recognition.face_encodings(rgb_face_img)\n",
    "                if face_encodings:\n",
    "                    face_encoding = face_encodings[0]\n",
    "\n",
    "                    if is_unique_face(face_encoding, known_faces):\n",
    "                        known_faces.append(face_encoding)\n",
    "                        face_count += 1\n",
    "                        face_filename = os.path.join(output_dir, f'SRC_{face_count:02d}.jpg')\n",
    "                        cv2.imwrite(face_filename, face_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "                        print(f\"Saved {face_filename}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() \n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    process_video('./demo_file/army.mp4', './demo_file/multispecific', margin=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from options.test_options import TestOptions\n",
    "\n",
    "# Temporarily clear sys.argv to avoid parsing any arguments\n",
    "original_argv = sys.argv\n",
    "sys.argv = [sys.argv[0]]  # Keep only the script name\n",
    "\n",
    "# Create TestOptions and initialize\n",
    "opt = TestOptions()\n",
    "opt.initialize()\n",
    "opt = opt.parse()  # This should return the parsed options\n",
    "\n",
    "# Restore original sys.argv\n",
    "sys.argv = original_argv\n",
    "\n",
    "# Manually set or override the options\n",
    "opt.multisepcific_dir = './demo_file/multispecific'\n",
    "opt.video_path = './demo_file/output_720p.mp4'\n",
    "opt.output_path = './output/out.mp4'\n",
    "opt.temp_path = './tmp'\n",
    "opt.Arc_path = './arcface_model/arcface_checkpoint.tar'\n",
    "opt.name = 'people'\n",
    "opt.isTrain = False\n",
    "opt.use_mask = True\n",
    "opt.crop_size = 224  # Set this to the default value or the one you need\n",
    "\n",
    "# Add any other options that are required by your script\n",
    "# opt.other_option = value\n",
    "\n",
    "crop_size = opt.crop_size\n",
    "\n",
    "# Print out the options to verify\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from models.models import create_model\n",
    "from options.test_options import TestOptions\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "\n",
    "# Assuming opt is already defined and initialized\n",
    "pic_specific = opt.pic_specific_path\n",
    "crop_size = opt.crop_size\n",
    "multisepcific_dir = opt.multisepcific_dir\n",
    "\n",
    "torch.nn.Module.dump_patches = True\n",
    "model = create_model(opt)\n",
    "model.eval()\n",
    "\n",
    "app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "app.prepare(ctx_id=0, det_thresh=0.6, det_size=(640, 640))\n",
    "\n",
    "# Directory to save images where face detection fails\n",
    "error_dir = './error_images'\n",
    "os.makedirs(error_dir, exist_ok=True)\n",
    "\n",
    "# The specific person to be swapped (source)\n",
    "source_specific_id_nonorm_list = []\n",
    "source_path = os.path.join(multisepcific_dir, 'SRC_*')\n",
    "source_specific_images_path = sorted(glob.glob(source_path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for source_specific_image_path in source_specific_images_path:\n",
    "        specific_person_whole = cv2.imread(source_specific_image_path)\n",
    "        if specific_person_whole is None:\n",
    "            print(f\"Error reading image: {source_specific_image_path}\")\n",
    "            continue\n",
    "\n",
    "        result = app.get(specific_person_whole, crop_size)\n",
    "        if result is None or len(result) != 2:\n",
    "            print(f\"Face detection failed for image: {source_specific_image_path}\")\n",
    "            error_image_path = os.path.join(error_dir, os.path.basename(source_specific_image_path))\n",
    "            cv2.imwrite(error_image_path, specific_person_whole)\n",
    "            continue\n",
    "\n",
    "        specific_person_align_crop, _ = result\n",
    "        specific_person_align_crop_pil = Image.fromarray(cv2.cvtColor(specific_person_align_crop[0], cv2.COLOR_BGR2RGB))\n",
    "        specific_person = transformer_Arcface(specific_person_align_crop_pil)\n",
    "        specific_person = specific_person.view(-1, specific_person.shape[0], specific_person.shape[1], specific_person.shape[2])\n",
    "        specific_person = specific_person.cuda()\n",
    "        specific_person_downsample = F.interpolate(specific_person, size=(112, 112))\n",
    "        specific_person_id_nonorm = model.netArc(specific_person_downsample)\n",
    "        source_specific_id_nonorm_list.append(specific_person_id_nonorm.clone())\n",
    "\n",
    "# Print the count of successfully processed source images\n",
    "print(f\"Successfully processed source images: {len(source_specific_id_nonorm_list)}\")\n",
    "\n",
    "# The person who provides id information (list)\n",
    "target_id_norm_list = []\n",
    "target_path = os.path.join(multisepcific_dir, 'DST_*')\n",
    "target_images_path = sorted(glob.glob(target_path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for target_image_path in target_images_path:\n",
    "        img_a_whole = cv2.imread(target_image_path)\n",
    "        if img_a_whole is None:\n",
    "            print(f\"Error reading image: {target_image_path}\")\n",
    "            continue\n",
    "\n",
    "        result = app.get(img_a_whole, crop_size)\n",
    "        if result is None or len(result) != 2:\n",
    "            print(f\"Face detection failed for image: {target_image_path}\")\n",
    "            error_image_path = os.path.join(error_dir, os.path.basename(target_image_path))\n",
    "            cv2.imwrite(error_image_path, img_a_whole)\n",
    "            continue\n",
    "\n",
    "        img_a_align_crop, _ = result\n",
    "        img_a_align_crop_pil = Image.fromarray(cv2.cvtColor(img_a_align_crop[0], cv2.COLOR_BGR2RGB))\n",
    "        img_a = transformer_Arcface(img_a_align_crop_pil)\n",
    "        img_id = img_a.view(-1, img_a.shape[0], img_a.shape[1], img_a.shape[2])\n",
    "        img_id = img_id.cuda()\n",
    "        img_id_downsample = F.interpolate(img_id, size=(112, 112))\n",
    "        latend_id = model.netArc(img_id_downsample)\n",
    "        latend_id = F.normalize(latend_id, p=2, dim=1)\n",
    "        target_id_norm_list.append(latend_id.clone())\n",
    "\n",
    "# Print the count of successfully processed target images\n",
    "print(f\"Successfully processed target images: {len(target_id_norm_list)}\")\n",
    "\n",
    "assert len(target_id_norm_list) == len(source_specific_id_nonorm_list), \"The number of images in source and target directory must be same !!!\"\n",
    "video_swap(opt.video_path, target_id_norm_list, source_specific_id_nonorm_list, opt.id_thres, model, app, opt.output_path, temp_results_dir=opt.temp_path, no_simswaplogo=True, use_mask=opt.use_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!DEBIAN_FRONTEND=noninteractive dpkg --configure -a\n",
    "\n",
    "# Set DEBIAN_FRONTEND to noninteractive to avoid prompts\n",
    "!DEBIAN_FRONTEND=noninteractive apt install git-all -y\n",
    "!DEBIAN_FRONTEND=noninteractive apt install curl -y\n",
    "!DEBIAN_FRONTEND=noninteractive apt-get install mesa-va-drivers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yachty66/facefusion.git\n",
    "%cd facefusion\n",
    "!pip install -r requirements.txt\n",
    "# Run the install and run scripts\n",
    "!python install.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py --frame-processors face_enhancer -t ../SimSwap/output/out.mp4 -o ./enhanced_out.mp4 --face-enhancer-model gfpgan_1.4 --face-enhancer-blend 80 --face-selector-mode many --headless"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
