{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yachty66/SimSwap.git\n",
    "!cd SimSwap && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install insightface==0.2.1 onnxruntime moviepy\n",
    "!pip install googledrivedownloader\n",
    "!pip install imageio==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"SimSwap\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader\n",
    "\n",
    "### it seems that google drive link may not be permenant, you can find this ID from our open url.\n",
    "# GoogleDriveDownloader.download_file_from_google_drive(file_id='1TLNdIufzwesDbyr_nVTR7Zrx9oRHLM_N',\n",
    "#                                     dest_path='./arcface_model/arcface_checkpoint.tar')\n",
    "# GoogleDriveDownloader.download_file_from_google_drive(file_id='1PXkRiBUYbu1xWpQyDEJvGKeqqUFthJcI',\n",
    "#                                     dest_path='./checkpoints.zip')\n",
    "\n",
    "!wget -P ./arcface_model https://github.com/neuralchen/SimSwap/releases/download/1.0/arcface_checkpoint.tar\n",
    "!wget https://github.com/neuralchen/SimSwap/releases/download/1.0/checkpoints.zip\n",
    "!unzip ./checkpoints.zip  -d ./checkpoints\n",
    "!wget -P ./parsing_model/checkpoint https://github.com/neuralchen/SimSwap/releases/download/1.0/79999_iter.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \"https://sh23tw.dm.files.1drv.com/y4mmGiIkNVigkSwOKDcV3nwMJulRGhbtHdkheehR5TArc52UjudUYNXAEvKCii2O5LAmzGCGK6IfleocxuDeoKxDZkNzDRSt4ZUlEt8GlSOpCXAFEkBwaZimtWGDRbpIGpb_pz9Nq5jATBQpezBS6G_UtspWTkgrXHHxhviV2nWy8APPx134zOZrUIbkSF6xnsqzs3uZ_SEX_m9Rey0ykpx9w\" -O antelope.zip\n",
    "!unzip ./antelope.zip -d ./insightface_func/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import fractions\n",
    "import numpy as npa\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from models.models import create_model\n",
    "from options.test_options import TestOptions\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from util.videoswap_multispecific import video_swap\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcm(a, b): return abs(a * b) / fractions.gcd(a, b) if a and b else 0\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transformer_Arcface = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!apt install cmake\n",
    "!pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import face_recognition\n",
    "\n",
    "\n",
    "# Define the model URL and local path\n",
    "model_url = 'https://github.com/facefusion/facefusion-assets/releases/download/models/yoloface_8n.onnx'\n",
    "model_path = './models/yoloface_8n.onnx'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "# Function to download the model\n",
    "def download_model(url, path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading model from {url}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(path, 'wb') as file, tqdm(\n",
    "            desc=path,\n",
    "            total=total_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(chunk_size=1024):\n",
    "                file.write(data)\n",
    "                bar.update(len(data))\n",
    "        print(\"Model downloaded successfully.\")\n",
    "    else:\n",
    "        print(\"Model already exists locally.\")\n",
    "\n",
    "# Download the model if it doesn't exist\n",
    "download_model(model_url, model_path)\n",
    "\n",
    "# Load the YOLOFace model\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "def preprocess(frame, input_size):\n",
    "    resized = cv2.resize(frame, input_size)\n",
    "    normalized = resized / 255.0\n",
    "    transposed = np.transpose(normalized, (2, 0, 1))\n",
    "    input_tensor = np.expand_dims(transposed, axis=0).astype(np.float32)\n",
    "    return input_tensor\n",
    "\n",
    "def postprocess(detections, frame_shape, input_size, conf_threshold=0.5, nms_threshold=0.4):\n",
    "    boxes, scores, landmarks = [], [], []\n",
    "    frame_height, frame_width = frame_shape[:2]\n",
    "    input_width, input_height = input_size\n",
    "\n",
    "    for detection in detections:\n",
    "        score = detection[4]\n",
    "        if score > conf_threshold:\n",
    "            center_x, center_y, width, height = detection[:4]\n",
    "            x1 = int((center_x - width / 2) * frame_width / input_width)\n",
    "            y1 = int((center_y - height / 2) * frame_height / input_height)\n",
    "            x2 = int((center_x + width / 2) * frame_width / input_width)\n",
    "            y2 = int((center_y + height / 2) * frame_height / input_height)\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            scores.append(score)\n",
    "            landmarks.append(detection[5:])\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, scores, conf_threshold, nms_threshold)\n",
    "    if len(indices) > 0:\n",
    "        indices = indices.flatten()\n",
    "    unique_boxes = [boxes[i] for i in indices]\n",
    "    unique_landmarks = [landmarks[i] for i in indices]\n",
    "    return unique_boxes, unique_landmarks\n",
    "\n",
    "def detect_faces(frame, input_size=(640, 640)):\n",
    "    input_tensor = preprocess(frame, input_size)\n",
    "    detections = session.run(None, {session.get_inputs()[0].name: input_tensor})[0]\n",
    "    detections = np.squeeze(detections).T\n",
    "    boxes, landmarks = postprocess(detections, frame.shape, input_size)\n",
    "    return boxes, landmarks\n",
    "\n",
    "def is_unique_face(face_encoding, known_faces, threshold=0.6):\n",
    "    matches = face_recognition.compare_faces(known_faces, face_encoding, tolerance=threshold)\n",
    "    return not any(matches)\n",
    "\n",
    "def process_video(video_path, output_dir, margin=0.4):  # Increased margin to 0.7 (70%)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error: Could not open video.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    known_faces = []\n",
    "    face_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        boxes, landmarks = detect_faces(frame)\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "\n",
    "            # Calculate margin\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            x1 = max(0, x1 - int(margin * width))\n",
    "            y1 = max(0, y1 - int(margin * height))\n",
    "            x2 = min(frame.shape[1], x2 + int(margin * width))\n",
    "            y2 = min(frame.shape[0], y2 + int(margin * height))\n",
    "\n",
    "            # Debug prints to verify margin adjustment\n",
    "            print(f\"Original box: ({box}), Margin: {margin}\")\n",
    "            print(f\"Adjusted box: ({x1}, {y1}, {x2}, {y2})\")\n",
    "\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "            rgb_face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Get face encodings\n",
    "            face_encodings = face_recognition.face_encodings(rgb_face_img)\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]\n",
    "\n",
    "                if is_unique_face(face_encoding, known_faces):\n",
    "                    known_faces.append(face_encoding)\n",
    "                    face_count += 1\n",
    "                    face_filename = os.path.join(output_dir, f'SRC_{face_count:02d}.jpg')\n",
    "                    cv2.imwrite(face_filename, face_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "                    print(f\"Saved {face_filename}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    process_video('./demo_file/output_720p.mp4', './demo_file/multispecific', margin=0.4)  # Increased margin to 0.7 (70%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from options.test_options import TestOptions\n",
    "\n",
    "# Temporarily clear sys.argv to avoid parsing any arguments\n",
    "original_argv = sys.argv\n",
    "sys.argv = [sys.argv[0]]  # Keep only the script name\n",
    "\n",
    "# Create TestOptions and initialize\n",
    "opt = TestOptions()\n",
    "opt.initialize()\n",
    "opt = opt.parse()  # This should return the parsed options\n",
    "\n",
    "# Restore original sys.argv\n",
    "sys.argv = original_argv\n",
    "\n",
    "# Manually set or override the options\n",
    "opt.multisepcific_dir = './demo_file/multispecific'\n",
    "opt.video_path = './demo_file/output_720p.mp4'\n",
    "opt.output_path = './output/out2.mp4'\n",
    "opt.temp_path = './tmp'\n",
    "opt.Arc_path = './arcface_model/arcface_checkpoint.tar'\n",
    "opt.name = 'people'\n",
    "opt.isTrain = False\n",
    "opt.use_mask = True\n",
    "opt.crop_size = 224  # Set this to the default value or the one you need\n",
    "\n",
    "# Add any other options that are required by your script\n",
    "# opt.other_option = value\n",
    "\n",
    "crop_size = opt.crop_size\n",
    "\n",
    "# Print out the options to verify\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from models.models import create_model\n",
    "from options.test_options import TestOptions\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "\n",
    "# Assuming opt is already defined and initialized\n",
    "pic_specific = opt.pic_specific_path\n",
    "crop_size = opt.crop_size\n",
    "multisepcific_dir = opt.multisepcific_dir\n",
    "\n",
    "torch.nn.Module.dump_patches = True\n",
    "model = create_model(opt)\n",
    "model.eval()\n",
    "\n",
    "app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "app.prepare(ctx_id=0, det_thresh=0.6, det_size=(640, 640))\n",
    "\n",
    "# Directory to save images where face detection fails\n",
    "error_dir = './error_images'\n",
    "os.makedirs(error_dir, exist_ok=True)\n",
    "\n",
    "# The specific person to be swapped (source)\n",
    "source_specific_id_nonorm_list = []\n",
    "source_path = os.path.join(multisepcific_dir, 'SRC_*')\n",
    "source_specific_images_path = sorted(glob.glob(source_path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for source_specific_image_path in source_specific_images_path:\n",
    "        specific_person_whole = cv2.imread(source_specific_image_path)\n",
    "        if specific_person_whole is None:\n",
    "            print(f\"Error reading image: {source_specific_image_path}\")\n",
    "            continue\n",
    "\n",
    "        result = app.get(specific_person_whole, crop_size)\n",
    "        if result is None or len(result) != 2:\n",
    "            print(f\"Face detection failed for image: {source_specific_image_path}\")\n",
    "            error_image_path = os.path.join(error_dir, os.path.basename(source_specific_image_path))\n",
    "            cv2.imwrite(error_image_path, specific_person_whole)\n",
    "            continue\n",
    "\n",
    "        specific_person_align_crop, _ = result\n",
    "        specific_person_align_crop_pil = Image.fromarray(cv2.cvtColor(specific_person_align_crop[0], cv2.COLOR_BGR2RGB))\n",
    "        specific_person = transformer_Arcface(specific_person_align_crop_pil)\n",
    "        specific_person = specific_person.view(-1, specific_person.shape[0], specific_person.shape[1], specific_person.shape[2])\n",
    "        specific_person = specific_person.cuda()\n",
    "        specific_person_downsample = F.interpolate(specific_person, size=(112, 112))\n",
    "        specific_person_id_nonorm = model.netArc(specific_person_downsample)\n",
    "        source_specific_id_nonorm_list.append(specific_person_id_nonorm.clone())\n",
    "\n",
    "# Print the count of successfully processed source images\n",
    "print(f\"Successfully processed source images: {len(source_specific_id_nonorm_list)}\")\n",
    "\n",
    "# The person who provides id information (list)\n",
    "target_id_norm_list = []\n",
    "target_path = os.path.join(multisepcific_dir, 'DST_*')\n",
    "target_images_path = sorted(glob.glob(target_path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for target_image_path in target_images_path:\n",
    "        img_a_whole = cv2.imread(target_image_path)\n",
    "        if img_a_whole is None:\n",
    "            print(f\"Error reading image: {target_image_path}\")\n",
    "            continue\n",
    "\n",
    "        result = app.get(img_a_whole, crop_size)\n",
    "        if result is None or len(result) != 2:\n",
    "            print(f\"Face detection failed for image: {target_image_path}\")\n",
    "            error_image_path = os.path.join(error_dir, os.path.basename(target_image_path))\n",
    "            cv2.imwrite(error_image_path, img_a_whole)\n",
    "            continue\n",
    "\n",
    "        img_a_align_crop, _ = result\n",
    "        img_a_align_crop_pil = Image.fromarray(cv2.cvtColor(img_a_align_crop[0], cv2.COLOR_BGR2RGB))\n",
    "        img_a = transformer_Arcface(img_a_align_crop_pil)\n",
    "        img_id = img_a.view(-1, img_a.shape[0], img_a.shape[1], img_a.shape[2])\n",
    "        img_id = img_id.cuda()\n",
    "        img_id_downsample = F.interpolate(img_id, size=(112, 112))\n",
    "        latend_id = model.netArc(img_id_downsample)\n",
    "        latend_id = F.normalize(latend_id, p=2, dim=1)\n",
    "        target_id_norm_list.append(latend_id.clone())\n",
    "\n",
    "# Print the count of successfully processed target images\n",
    "print(f\"Successfully processed target images: {len(target_id_norm_list)}\")\n",
    "\n",
    "assert len(target_id_norm_list) == len(source_specific_id_nonorm_list), \"The number of images in source and target directory must be same !!!\"\n",
    "video_swap(opt.video_path, target_id_norm_list, source_specific_id_nonorm_list, opt.id_thres, model, app, opt.output_path, temp_results_dir=opt.temp_path, no_simswaplogo=opt.no_simswaplogo, use_mask=opt.use_mask)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
